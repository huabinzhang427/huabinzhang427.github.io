{"meta":{"title":"张华宾的博客","subtitle":"先博而后渊","description":null,"author":"ZhangHuaBin","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"数据结构中数组与编程语言中数组的关系","slug":"array-differ","date":"2019-05-23T07:24:45.000Z","updated":"2019-05-23T09:50:53.993Z","comments":false,"path":"post/array-differ/","link":"http://yoursite.com/post/array-differ/","permalink":"http://yoursite.com/post/array-differ/","excerpt":"数据结构与算法中的“数组”与编程语言中的“数组”，它们并不完全等同。编程语言在实现自己的“数组”类型的时候，并不是完全遵循数据结构“数组”的定义，而是针对编程语言自身的特点，做了调整。","text":"数据结构与算法中的“数组”与编程语言中的“数组”，它们并不完全等同。编程语言在实现自己的“数组”类型的时候，并不是完全遵循数据结构“数组”的定义，而是针对编程语言自身的特点，做了调整。 数据结构与算法中的数组概念是这样定义的：数组是存储相同数据类型的一块连续的存储空间。也就是，数组中的数据必须是相同类型的，数组中的数据必须是连续存储的。只有这样，数组才能实现根据下标快速地（时间复杂度是 O(1)）定位一个元素。 在不同的编程语言中，数组这种数据类型的实现方式都不大相同，比如下面的 C/C++、Java、Javascript C/C++ 中数组的实现方式C/C++ 中的数组，是非常标准的数据结构中的数组，也就是连续存储相同类型的数据的一块内存空间。 在 C/C++ 中，不管是基本类型数据，比如 int、long、char，还是结构体、对象，在数组中都是连续存储的。 基本数据类型数组 1234int arr[3];arr[0] = 0;arr[1] = 1;arr[2] = 2; 数组 arr 中存储的是 int 基本类型的数据，对应的内存存储方式。从图中可以看出，数据是存储在一片连续的内存空间中。 结构体（class 对象）数组 123456789101112struct Dog &#123; char a; char b;&#125;struct Dog arr[3];arr[0].a = '0';arr[0].b = '1';arr[1].a = '2';arr[1].b = '3';arr[2].a = '4';arr[2].b = '5'; 结构体数组中的元素，也是存储在一片连续的内存空间中。 二维数组（或多维数组） 12345struct Dog &#123; char a; char b;&#125;struct Dog arr[3][2]; 和数据结构中二维数组是一样的，数据先按行后按列，并且是连续存储的。 C/C++ 中数组的数据存储方式，完全符合数据结构和算法中数组的定义。 Java 中数组的实现方式 基本数据类型数组 1234int arr[] = new int[3];arr[0] = 1;arr[1] = 2;arr[2] = 3; 注意，new 申请的空间在堆上，arr 存储在栈上。arr 存储的是数组空间的首地址。 在 java 中，基本数据类型数组还是符合数据结构中数组的定义的。数组中数据是相同类型的、并且存储在一片连续的内存空间中。 class 对象数组 1234567891011class Person &#123; private String name; public Person(String name) &#123; this.name = name; &#125;&#125;Person arr[] = new Person[3];arr[0] = new Person('0');arr[1] = new Person('1');arr[2] = new Person('2'); 在 Java 中，对象数组存储的是对象在内存中的地址，而非对象本身。对象本身在内存中并不是连续存储的，而是散落在各个地方。 二维数组（或多维数组） 二维数组（或多维数组）存储基本数据类型 1234int arr[][] = new int[3][];arr[0] = new int[1];arr[1] = new int[2];arr[2] = new int[3]; 在 java 中，二维数组中的第二维，可以是不同长度的。 二维数组（或多维数组）存储对象 1234567Person arr[] = new Person[3][];arr[0] = new Person[1];arr[1] = new Person[2];arr[2] = new Person[3];arr[0][0] = new Person('0');arr[1][1] = new Person('1'); 在 Java 编程语言中，数组这种数据类型，除了存储基本类型的一维数组之外，对象数组、二维数组都和数据结构中数组的定义有很大的区别。 Javascript 中数组的实现方式JavaScript 中的数组，可以存储不同类型的数据，数组中的数据也不一定是连续存储的（按照下标随机访问的效率不高），并且还能支持变长数组。 实际上，Javascript 中的数组，会根据我们存储数据的不同，选择不同的实现方式。如果数组中存储的是相同类型的数据，那 JavaScript 就用数据结构中数组来实现，也就是说，会分配一块连续的内存空间来存储数据。如果数组中存储的是非相同类型的数据，那 JavaScript 就用类似于散列表的结构来存储数据。也就是说，数据并不是连续存储在内存中的。这也是 JavaScript 数组支持存储不同类型数据的原因。 如果我们从一个存储了相同类型数据的数组中，插入一个不同类型的数据，那 JavaScript 会将底层的数据结构，从数组变成散列表。 JavaScript 为了照顾一些底层应用的开发者，提供了另一种数据类型 ArrayBuffer，它符合标准的数据结构中数组的定义，它分配一片连续的内存空间，仅仅用来存储相同类型的数据。 内容总结数据结构和算法先于编程语言出现，编程语言中的一些数据类型，并不能跟数据结构和算法中数组的定义一一对应，它们往往会根据自身语言的特点，在实现上做一些调整。","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"},{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"},{"name":"数组","slug":"数组","permalink":"http://yoursite.com/tags/数组/"}]},{"title":"数组-最基础的数据结构","slug":"array-analysis","date":"2019-05-13T01:09:47.000Z","updated":"2019-05-13T09:03:15.599Z","comments":false,"path":"post/array-analysis/","link":"http://yoursite.com/post/array-analysis/","permalink":"http://yoursite.com/post/array-analysis/","excerpt":"数组（Array）是一种线性表数据结构，它用一组连续的内存空间，来存储一组具有相同类型的数据。","text":"数组（Array）是一种线性表数据结构，它用一组连续的内存空间，来存储一组具有相同类型的数据。 为什么很多编程语言中数组都从 0 开始编号？ 从 1 开始不是更符合人类的思维习惯吗？ 数组如何实现随机访问 什么是数组？ 数组（Array）是一种线性表数据结构，它用一组连续的内存空间，来存储一组具有相同类型的数据。 线性表（Linear List）线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向，除了数组，链表、队列、栈等也是线性表结构。 而与它相对立的概念就是非线性表。在非线性表中，数据之间并不是简单的前后关系。比如二叉树、堆、图等。 连续的内存空间和相同类型的数据正是因为有了这两个限制，它才有了一个“杀手锏”的特性：“随机访问”。 数组是如何实现根据下标随机访问的数组元素的？ 上图中，计算机给数组 a[10] 分配了一块连续内存空间 1000~1039，其中，内存块的首地址为 base_address = 1000。 计算机会给每个内存单元（比如定义的数组 a[10]）分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算该元素存储的内存地址： a[i]_address = base_address + i * data_type_size 其中 data_type_size 表示数组中每个元素的大小（元素数据类型的字节大小）。比如上面例子中，数组中存储的是 int 类型的数据，所以 data_type_size 就是 4 个字节。 数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。 数组的连续的内存空间和相同类型的数据特性有利也有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。 低效的“插入”和“删除” 为什么会导致低效？又有哪些改进的方法？ 插入操作场景：假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置，为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k~n 这部分的元素都顺序地往后挪一位。 提问：上面插入操作地时间复杂度是多少呢？ 分析： 如果在数组的末尾插入元素，那就不需要移动数据了，这时的最好时间复杂度为 O(1)； 但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度为 O(n)； 因为我们在每个位置插入元素的概率是一样的 1/n，所以平均情况时间复杂度为 (1+2+3+…+n)/n = O(n)。 改进： 如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。 但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数组插入到第 k 个位置，为了避免大规模的数据搬移，我们可以直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。 在特定的场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。这个处理思想在快排中也会用到。 删除操作跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。 改进： 为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正地删除操作，这样就大大减少了删除操作导致的数据搬移。 上面就是 JVM 标记清除垃圾回收算法的核心思想。它指的是大多数主流虚拟机采用可达性分析算法来判断对象是否存活，在标记阶段，会遍历所有 GC ROOTS，将所有 GC ROOTS 可达的对象标记为存。只有当标记工作完成之后清理工作才会开始。 很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。 警惕数组的访问越界问题Java 本身会做越界检查，抛出 ‘java.lang.ArrayIndexOutOfBoundsException’。 容器能否完全替代数组针对数组类型，很多语言都提供了容器类。比如 Java 中的 ArrayList。 在项目开发中，什么时候适合数组？什么时候适合用容器呢？ ArrayList 最大的优势就是可以将很多数组操作的细节封装起来。比如数组插入、删除数据时需要搬移其它数据等。另外，它还有一个优势，就是支持动态扩容。 数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果我们申请了大小为 10 的数组，当第 11 个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。 如果使用 ArrayList，我们就完全不需要关心底层扩容逻辑，ArrayList 已经帮我们实现好了。每次存储空间不够的时候，它都会将空间自动扩容 1.5 倍大小。 但是，需要注意的是扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。 1234ArrayList&lt;User&gt; users = new ArrayList(10000)for(int i = 0; i &lt; 10000; ++i) &#123; users.add(xxx)&#125; 经验总结 Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 又有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组； 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以选用数组； 当要表示多维数组时，用数组往往会更加直观。比如 array [] []，而用容器的话需要这样定义：ArrayList array。 解答开篇 为什么很多编程语言中数组都从 0 开始编号？ 从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（Offset）”。如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用如下公式： a[k]_address = base_address + k * type_size 但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就变为： a[k]_address = base_address + (k - 1)*type_size 数组作为非常基础的数据结构，通过下标随机访问数组元素又是非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始（原因有些勉强，历史原因）。 C 语言设计者用 0 开始计算数组下标，之后的 Java、JavaScript 等高级语言都效仿了 C 语言，或者说，为了在一定程度上减少 C 语言程序员学习 Java 的学习成本，因此继续沿用了从 0 开始计数的习惯。实际上，还有一些语言并不是从 0 开始计数的，甚至支持负数下标，比如 Python。 内容总结数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为 O(n)。在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果特别底层的开发，直接使用数组可能会更合适。","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"},{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"},{"name":"数组","slug":"数组","permalink":"http://yoursite.com/tags/数组/"}]},{"title":"编程语言的进化","slug":"programe-language-evolution","date":"2019-05-09T12:18:10.000Z","updated":"2019-05-09T15:04:40.242Z","comments":false,"path":"post/programe-language-evolution/","link":"http://yoursite.com/post/programe-language-evolution/","permalink":"http://yoursite.com/post/programe-language-evolution/","excerpt":"从编程范式演进的角度来谈编程语言的进化。包括过程式、函数式、面向对象、面向连接。","text":"从编程范式演进的角度来谈编程语言的进化。包括过程式、函数式、面向对象、面向连接。 编程语言在信息科技发展中的位置，如同人类文明中语言所处的位置。而编程语言写出来的软件（及其源代码），如同人类文明中不断被继承下来的图书典籍。 如果你把编程语言升华为人类知识传承能力的进化，你就能清晰地预判到这样地未来：每一个小孩地基础教育中一定会有编程教育，就如同每一个小孩都需要学习物理和数学一样。 编程范式的进化编程语言从汇编开始，迭代至今，产生了一些编程范式。 过程式过程式就是以一条条命令的方式，让计算机按照我们的意愿来执行。过程式语言的代表有 C/C++、JavaScript、Go等。 过程式编程中最核心的两个概念： 结构体（自定义的类型），通过结构体对数据进行组合，可以构建出任意复杂的自定义数据结构； 过程（函数），通过过程可以抽象出任意复杂的自定义指令，复用以前的，简化意图的表达。 函数式函数式本质上是过程式编程的一种约束，它最核心的主张就是变量不可变，函数尽可能没有副作用（对于通用语言来说，所有函数都没有副作用是不可能的，内部有 IO 行为的函数就有副作用）。 既然变量不可变，函数没有副作用，自然人们犯错的机会也就更少，代码质量就会更高。大部分语言会比较难以彻底实施函数式的编程思想，但在思想上会有所借鉴。 函数式编程相对小众，因为这样写代码虽然质量高，但是学习门槛也高。 面向对象面向对象在过程式的基础上，引入了对象（类）和对象方法（类成员函数），它主张尽可能把方法（其实就是过程）归纳到合适的对象（类）上，不主张全局函数（过程）。面向对象的语言代表有 Java、C#、C++、Go等等。 从“面向对象”到“面向连接”面向对象的核心思想是引入契约，基于对象这样一个概念对代码的使用界面进行抽象和封装。它有两个显著的优点： 清晰的使用界面，某种类型的对象有哪些方法一目了然，而不像过程式编程，数据结构和过程的关系是非常松散的。 信息的封装，面向对象不主张绕过对象的使用接口侵入到对象的内部实现细节，因为这样做破坏了信息的封装，降低了类的可复用性，有一天对象的内部实现方式改变了，依赖该对象的相关代码也需要跟着调整。 面向对象还有一个至关重要的概念是“接口”，通过接口，我们可以优雅地实现过程式编程中很费劲才能做到地一个能力：多态。 由于对象和对象方法地强关联，我们可以引入接口来抽象不同对象相同地行为（比如鸟和猪是不同的而对象，但是它们有相同的方法，比如移动和吃东西），这样不同对象就可以用相同的代码来实现类似的复杂行为，这就是多态了。 多数面向对象语言还会引入一个叫继承（过度设计）的概念。虽然继承带来了编码上的便捷性，但也带来了不必要的心智负担：本来复合对象的唯一构造方法是组合，现在多了一个选择，继承。 不同编程范式并不是互斥的。虽然有些语言会有明确的编程范式主张，比如 Java 是纯正的面向对象语言，它反对全局过程。但是，也有一些语言明确主张说自己是多范式的，典型代表 C++。 可能 Go 语言是多范式更好的例子。它没有声称自己是多范式，但是实际上每一种编程范式它都保留了精华部分。这并没有使得 Go 语言变得很复杂，整个语言的特性及其精简。Go 是一门面向连接的语言。 什么是面向连接的语言？ 所谓面向连接就是朴素的组合思想。研究连接，就是研究人与人如何组合，研究代码与代码之间怎么组合。架构思维的核心法则也是组合。 面向对象创造性的把契约的重要性提高到了非常重要的高度，但这还远远不够。这是因为，并不是只有对象需要契约，语言设计的方方面面都需要契约。比如代码规范约束了人的行为，是人与人的连接契约。消息传递（多核背景下的一种编程思想，其核心主张是：尽可能用消息传递来取代共享内存，从而尽可能避免显式的锁，降低编程负担）约束了进程的行为，是进程与进程的连接契约。 其它方面的进化除了编程范式，编程语言的进化还体现在工程化能力的完善上，过程化能力主要体现在如下方面： 包（package），代码的发布单元 版本（version），包的依赖管理 文档生成（doc） 单元测试（test） 从语言执行器的行为看，出现了三种分类的语言： 编译的目标文件为可执行程序，如 C/C++、Go等 生成跨平台的虚拟机字节码，有独立的执行器（虚拟机）执行字节码，如 Java 直接解释执行，典型代表是 JavaScript。 语言对架构的影响 从架构设计的角度，编程语言的选择对架构的影响是什么？ 软件的架构包括一部分是业务无关的框架和基础库，还有一部分是业务架构。 从软件的业务架构来说本身应该怎么拆分模块，每个模块具体做什么事情（业务边界是什么），这是业务需求本身决定的，和编程语言无关。但在我们描述每个模块的规格时，采用的规格描述语言会面临两种选择： 选择某种语言无关的接口表示； 选择团队开发时采用的语言来描述接口。 两种选择的差异并不是实质性的，只要团队内有共识，选哪一种都无所谓。“如无必要勿增实体”的原则，倾向用开发语言来做接口表示。 站在唯技术论的角度，业务架构和语言无关，影响的只是模块规格的描述语法。但语言的选择在实践中对业务架构决策的影响仍然极其关键，因为考虑开发效率和后期维护。 内容总结从编程范式演进的角度来谈编程语言的进化。包括过程式、函数式、面向对象、面向连接。 编程框架和编程范式的区别？ 编程框架通常是领域性的，它是一个或多个包（package），往往规定了某个领域的业务你应该怎么写；而编程范式是普适性的，不管解决什么领域的问题都可以适用。","categories":[],"tags":[{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"},{"name":"架构设计","slug":"架构设计","permalink":"http://yoursite.com/tags/架构设计/"}]},{"title":"分析冯·诺依曼体系结构","slug":"framework-von-neumann","date":"2019-05-08T01:40:48.000Z","updated":"2019-05-08T07:37:46.213Z","comments":false,"path":"post/framework-von-neumann/","link":"http://yoursite.com/post/framework-von-neumann/","permalink":"http://yoursite.com/post/framework-von-neumann/","excerpt":"冯·诺依曼体系结构的不凡之处在于，它想“解决一切可以用‘计算’来解决的问题”。","text":"冯·诺依曼体系结构的不凡之处在于，它想“解决一切可以用‘计算’来解决的问题”。 分析架构的关键点 我们应该如何去分析架构设计中涉及的每一个零部件？ 换句话说，当我们设计或分析一个零部件时，我们会关心哪些问题？ 需求这个零部件的作用是什么？它能用来做哪些事情？它不会被用来做哪些事情？ 为何这个零部件被设计用来干这些事情，而不是多干一点事情，或少干某些事情？ 规格这个零部件的接口是什么样的？它如何与其他零部件连接在一起的？ 规格是零部件的连接需求的抽象。符合规格的零部件可以有非常多种可能的实现方案，但是，一旦规格中的某个条件不能满足了，它就无法正常完成与其他零部件的连接，以达到预期的需求目标。 规格的约束条件是非常多样化的。可能是外观（如形状和颜色）、交互方式（如键盘、鼠标、语音、触摸屏）、质量（硬度、耐热性等）。 那么，冯·诺依曼体系结构的需求和规格又是什么呢？ 冯·诺依曼体系结构分析冯·诺依曼体系结构不但是应用程序这座大厦的地基，同时也是整个信息科技的地基。事实上，它更像是一个无中生有的全新世界：在其中，有个体、有族群、有生态，还有喜怒哀乐。 从需求来说，它想解决一切问题。解决一切可以用“计算”来解决的问题。 为了实现“解决一切可以用‘计算’来解决的问题”这个目标，冯·诺依曼引入了三类基础零部件： 中央处理器 存储 输入输出设备 存储，它负责存放计算涉及的相关数据，作为计算的输入参数和输出结果。常见的存储设备非常多样化，比如：中央处理器内置的寄存器、内存、传统机械硬盘、USB 固态硬盘等等。 从中央处理器的角度，存储见到那分为两类： 内置支持的存储（常规的处理器指令可直接访问），比如寄存器、内存、计算机主板的 ROM。 外置存储（中央处理器本身并不能直接读写其中的数据），输入输出设备。 冯·诺依曼体系涉及的“存储”，指的是中央处理器内置支持的存储。 输入输出设备，每个设备通过一个端口与中央处理器连接。通过这个端口地址，中央处理器可以和设备进行数据交换。 数据交换设计的数据格式由设备定义，中央处理器并不理解。设备数据交换的发起方（设备使用方）通常理解并可以解释所接收的数据含义。为了方便使用，设备厂商或操作系统厂商通常会提供设备相关的驱动程序，把设备交换的细节隐藏起来，设备的使用方只需要调用相关的接口函数就可以操作设备。 中央处理器，它负责程序（指令序列）的执行。指令序列也存放在存储里面，计算机加电启动后，中央处理器会从一个固定的存储地址开始执行。通常这个固定的存储地址指向计算机主板的 ROM 上的一段启动程序（BIOS），这段启动程序通常包含以下内容： 存储设备的驱动程序，用以识别常规的外置存储设备，比如硬盘、光驱、U 盘； 基础外部设备的驱动程序，比如键盘、鼠标、显示器（显卡）； 设备和启动配置的基础管理能力； 在外置存储上执行程序的能力； 将执行权转移到外置存储上的操作系统启动程序，这样操作系统就开始干活了。 “计算”需求的多样性只需要通过调整计算机主板上的 BIOS 程序，乃至外置存储中的操作系统启动程序就可以实现了，而不必去修改中央处理器本身。 中央处理器支持的指令分类： 计算类，各类数学运算； I/O 类，从存储读写数据，从输入输出设备读写数据； 指令跳转类，在满足特定条件下跳转到新的当前程序执行位置、调用自定义函数。 和“解决一切可以用‘计算’来解决的问题”这个伟大的目标相比，冯·诺依曼体系的三类零部件的规格设计显得如此精简。 为什么这么简洁的规格设计，居然可以解决这么复杂的需求？ 需求是怎么被满足的？对于“解决一切可以用‘计算’来解决的问题”这样一个模糊的需求，如果你是“电脑”这个产品的主架构师，你会如何应对？ 分析思路如下： 一方面，需求的变化点在于，要解决的问题是五花八门包罗万象的。如何以某种稳定但可扩展的架构来支持这样的变化呢？ 另一方面，需求的稳定点在于，电脑的核心能力是固定的，怎么表达电脑的核心能力？ 电脑的核心能力是“计算”。 什么是计算？计算就是对一个数据（输入）进行变换，变为另一个数据（输出）。在数学中把它叫作“函数”。 y = f(x) x、y 是数据，可以是简单的数值，也可能是文本、图片、视频，各种我们对现实问题进行参数化建模后的测量值，当然也可能是多个输入数据。但无论它的逻辑含义是什么，物理上都可以以一段连续的字节内容来表达。 x、y 物理上在哪里？ “存储”这个概念就产生了。存储就是存放计算所要操作数据的所在。 一个具体的计算（函数）怎么表达？ 计算（函数）对于电脑的架构师来说是未知的，那么怎么设计一种系统架构让用户可以表达任意复杂的计算（函数）？ 逻辑上来看，无论多么复杂的自定义函数，都可以通过下面这些元素的组合来定义： 内置函数，整数或小数运算 循环和条件分支 子函数（自定义函数） 因此，对于任意的具体的计算（自定义函数）来说，都可以用一组指令序列来表达。 那么函数物理上在哪里？ 函数以指令序列形式存放在存储里面。也就是说，存储不只存放计算索要操作的数据，也存放“计算”本身。 只是，存储里面存放的“计算”只是数据，需要有人理解并执行这些数据背后的计算行为，才变成真正意义的“计算”。这个执行者，就是中央处理器（CPU），它支持很多计算指令，包括执行内置函数、循环和条件分支、执行子函数等。 电脑有了“中央处理器+存储”，即有了计算的能力和计算的数据，但无法和现实世界发生交互。 交互，抽象来看就是输入和输出。对于电脑来说，输入和输出的需求多样化。 从输入需求来说，可能采集静态图像、声音、视频；也可能采集结构化的数据，如 GPS 位置、脉搏、心电图、温度、湿度等；还可能是用户控制指令，如键盘按键、鼠标、触摸屏动作等。 从输出需求来说，可能像屏幕输出信息；播放声音；执行某个动作如交通灯开关、打印机打印等。 不管是什么样交互用途的设备，我们要做的只是定义好统一的数据交换协议？ 因此中央处理器不仅要有“计算”能力，还要有“数据交换”能力（IO 能力）。 输入输出设备从根本上解决了什么问题？ 是电脑无限扩展的能力。最重要的是，输入输出设备和电脑是完全异构的。输入输出设备对电脑来说就只是实现了某项功能的黑匣子。如果把交互能力也看作是一种计算能力的话，电脑理论上能够解决的“计算”问题变得无所不包。 内容总结架构的第一步是需求分析，从需求分析角度来说，关键要抓住需求的稳定点和变化点。需求的稳定点，往往是系统的核心价值点；而需求的变化点，则往往需要相应去做开放性设计。 对于电脑这个产品来说，需求的稳定点是电脑的 “计算”能力 ，需求的变化点，一是用户 “计算”需求 的多样性，二是用户交互方式的多样性。 电脑的“计算”能力，最终体现为中央处理器的指令集，这是需求相对稳定的部分。 用户“计算”需求的多样性，只需要通过调整计算机主板上的 BIOS 程序，乃至外置存储中的操作系统启动程序来实现，而不必去修改中央处理器本身。 用户交互方式的多样性，则通过定义外部设备与中央处理器的数据交换协议实现。 分析需求的稳定点和变化点 描述需求需要几个典型的要素： 用户，面向的人群； 他们有什么要解决的问题； 解决这个问题的核心系统 只有满足了上面几个要素的需求才能进一步讨论变化点和稳定点。 存储让数据跨越时间，传输让数据跨越空间，计算让数据改变形式。","categories":[],"tags":[{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"},{"name":"架构设计","slug":"架构设计","permalink":"http://yoursite.com/tags/架构设计/"}]},{"title":"最好/最坏/平均/均摊时间复杂度分析","slug":"complexity-analysis2","date":"2019-05-06T07:13:59.000Z","updated":"2019-05-06T10:54:05.373Z","comments":false,"path":"post/complexity-analysis2/","link":"http://yoursite.com/post/complexity-analysis2/","permalink":"http://yoursite.com/post/complexity-analysis2/","excerpt":"同一段代码，在不同输入的情况下，复杂度量级可能是不一样的。","text":"同一段代码，在不同输入的情况下，复杂度量级可能是不一样的。 最好情况时间复杂度（best case time complexity） 最坏情况时间复杂度（worst case time complexity） 平均情况时间复杂度（average case time complexity） 均摊时间复杂度（amoritized time complexity） 基础复杂度分析回顾12345678910// 示例1// n 表示 array 长度int find(int[] array, int n, int x) &#123; int i = 0; int pos = -1; for(; i &lt; n; ++i) &#123; if(array[i] == x) pos = i; &#125; return pos;&#125; 上面示例1代码的功能是，在一个无序的数组（array）中，查找变量 x 出现的位置，如果没有找到就返回 -1。通过之前的分析方法，该代码算法的时间复杂度是 O(n)。 我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环。所以优化示例1中的代码得到： 12345678910111213// 示例2// n 表示 array 长度int find(int[] array, int n, int x) &#123; int i = 0; int pos = -1; for(; i &lt; n; ++i) &#123; if(array[i] == x) &#123; pos = i; break; &#125; &#125; return pos;&#125; 我们优化完代码后，示例2中的时间复杂度还是 O(n) 吗？ 分析：因为我们要查找的变量 x 可能出现在数组的任意位置。如果数组中的第一个元素正好是要查找的变量 x，那就不需要再继续遍历剩下的 n-1 个数据了，那时间复杂度就是 O(1)。但如果数组中不存在变量 x，那我们就需要把整个数组都遍历一遍，时间复杂度就变成了 O(n)。所以不同的情况下，示例2中的时间复杂度是不一样的。 最好情况时间复杂度最好情况时间复杂度，就是在最理想的情况下，执行代码的时间复杂度。 在示例2中，在最理想的情况下，要查找的变量 x 正好是数组的第一个元素，这个时候对应的时间复杂度 O(1) 就是最好情况时间复杂度。 最坏情况时间复杂度最坏情况时间复杂度，就是在最糟糕的情况下，执行代码的时间复杂度。 在示例2中，在最坏的情况下，要查找的变量 x 不在数组中，我们需要把整个数组都遍历一遍才行，这个时候对应的时间复杂度 O(n) 就是最好情况时间复杂度。 平均情况时间复杂度我们都知道最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码时间复杂度，发生的概率其实并不大。所以引入平均情况时间复杂度，简称平均时间复杂度。 平均时间复杂度怎么分析? 在示例2中，我们要查找的变量 x 在数组中的位置，有 n+1 种情况： 在数组的 0~n-1 位置中 不在数组中 我们把每种情况下，查找需要遍历的元素个数累加起来,然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即： (1+2+3...+n+n)/(n+1) = n(n+3)/2(n+1) 根据时间复杂度大 O 表示法，上面的公式简化后，得到的平均复杂度就是 O(n)。 上面的 n+1 种情况出现的概率并不是一样的，又该怎么处理？ 我们假设在数组中和不在数组中的概率都是 1/2，另外要查找的数据出现在 0~n-1 这 n 个位置的概率是一样的，为 1/n。所以，根据概率乘法原则，要查找的数据出现在 0~n-1 中任意位置的概率就是 1/(2n)。 把各种情况发生的概率考虑进去，那么平均复杂度的计算过程变成了： (1+2+3...+n)*(1/2n)+n*(1/2) = (3n+1)/4 上面的计算结果就是加权平均值（期望值），所以平均时间复杂度的全称叫加权平均时间复杂度或期望时间复杂度。 引入概率后，最终示例2代码的加权平均时间复杂度仍然是 O(n)。 实际上在大多数情况下，我们不需要区分最好、最坏、平均情况时间复杂度三种情况，使用一个复杂度就可以满足要求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距时，才会使用这三种复杂度表示来区分。 均摊时间复杂度12345678910111213141516// 示例3int[] array = new int[n];int count = 0;void insert(int val) &#123; if(count == array.length) &#123; int sum = 0; for(int i = 0; i &lt; array.length; ++i) &#123; sum = sum + array[i]; &#125; array[0] = sum; count = 1; &#125; array[count] = val; ++count;&#125; 上面示例3代码的功能是往数组中插入数据。当数组满了之后，用 for 循环遍历数组求和，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据依次插入。 示例3代码的时间复杂度分析如下： 最理想的情况下，数组中有空闲空间，所以最好情况时间复杂度为 O(1)。最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为 O(n)。 平均时间复杂度呢？ 假设数组的长度为 n，根据数据插入的位置的不同，我们可以分为 n 种情况，每种情况的时间复杂度是 O(1)，除此之外，还有一种情况是，在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是 O(n)，而且这 n+1 种情况发生的概率一样，都是 1/(n+1)。所以根据加权平均的计算方法，得到的期望时间复杂度是： (1+1+1...+n)/(n+1) = O(1) 示例3代码的平均时间复杂度不需要引入概率？ 对比示例2代码中的 find() 函数和示例3代码中的 insert() 函数 find() 函数在极端情况下，复杂度才为 O(1)，但 insert() 函数在大部分情况下，时间复杂度都为 O(1)，只要个别情况下复杂度才比较高，为 O(n)。 insert() 函数，O(1) 复杂度的插入和 O(n) 时间复杂度的插入，出现的频率非常有规律，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。 针对这样的特殊情况，我们引入一种更加简单的分析方法：摊还分析法，通过该分析方法得到的时间复杂度叫均摊时间复杂度。 如何使用摊还分析法来分析算法的均摊时间复杂度？ 继续上面示例3中的代码，每一次 O(n) 的插入操作，都会跟着 n-1 次的 O(1) 的插入操作。所以把耗时多的那次操作均摊到接下来的 n-1 次耗时操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一起分析，看是否能将较高时间复杂度那次操作的耗时，平均到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度的分析场合，一般均摊时间复杂度就等于最好情况时间复杂度。 均摊时间复杂度是一种特殊的平均时间复杂度。 内容小结因为同一段代码，在不同的输入情况下，复杂度量级有可能是不一样的。所以引入了最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度和均摊时间复杂度。 平均情况时间复杂度和均摊时间复杂度基本上是同一个概念，均摊是特殊的平均。在分析时间复杂度是 O(1) 还是 O(n) 的时候，出现 O(1) 的次数远大于出现 O(n) 出现的次数时，平均情况/摊还时间复杂度就是 O(1)。","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"},{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"},{"name":"复杂度分析","slug":"复杂度分析","permalink":"http://yoursite.com/tags/复杂度分析/"}]},{"title":"基础复杂度分析","slug":"complexity-analysis","date":"2019-05-06T02:51:41.000Z","updated":"2019-05-06T07:25:22.452Z","comments":false,"path":"post/complexity-analysis/","link":"http://yoursite.com/post/complexity-analysis/","permalink":"http://yoursite.com/post/complexity-analysis/","excerpt":"数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行的更快，如何让代码更省存储空间。","text":"数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行的更快，如何让代码更省存储空间。 概述说到数据结构和算法，就一定离不开时间、空间复杂度分析。复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。 为什么需要复杂度分析？ 我们把代码跑一遍，通过统计、监控、就能得到算法执行的时间和占用的内存大小。为什么还需要做时间、空间复杂度分析呢？ 上面说的统计、监控的分析方法也叫作事后统计法，这种统计方法由很大的局限性。 测试结果非常依赖测试环境 测试结果受数据规模的影响很大 我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是下面要介绍的时间、空间复杂度分析方法。 大 O 复杂度表示法算法的执行效率，粗略的的讲，就是算法代码执行的时间。 123456789// 示例1int cal(int n) &#123; int sum = 0; int i = 1; for(; i &lt;= n; ++i) &#123; sum = sum + i; &#125; return sum;&#125; 从 CPU 的角度来看，上面代码的每一行执行着类似的操作：读数据-运算-写数据。假设每行代码执行的时间一样长，所有代码的执行时间 T(n) 与每行代码的执行次数成正比。 大 O 公式T(n) = O(f(n)) T(n)：代码执行的总时间； n：数据规模的大小； f(n)：每行代码执行的次数总和。 所以示例1中代码的大 O 时间复杂度可表示为 T(n) = O(2n + 2)。 大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势。所以，也叫作渐进时间复杂度(asymptotic time complexity)，简称时间复杂度。 当 n 很大时，公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略，我们只需要记录一个最大量级的就可以了。 时间复杂度分析 如何分析一段代码的时间复杂度？ 只需关注循环执行次数最多的一段代码 123456789// 示例2int cal(int n) &#123; int sum = 0; iny i = 1; for(; i &lt;= n; ++i) &#123; sum = sum + 1; &#125; return sum;&#125; 示例2中，第2、3行代码都是常量级的执行时间，与 n 的大小无关，所以对于复杂度没有影响。循环次数最多的是第4、5行代码，这两行代码被执行了 n 次，所以总的时间复杂度就是 O(n)。 加法法则 加法法则，总复杂度等于量级最大的那段代码的复杂度。 12345678910111213141516171819202122232425// 示例3int cal(int n) &#123; int sum_1 = 0; int p = 1; for(; p &lt; 100; ++p) &#123; sum_1 = sum_1 + p; &#125; int sum_2 = 0; int q = 1; for(; q &lt; 100; ++q) &#123; sum_2 = sum_2 + q; &#125; int sum_3 = 0; int i = 1; int j = 1; for(; i &lt; 100; ++i) &#123; int j = 1; for(; j &lt; 100; ++j) &#123; sum_3 = sum_3 + i * j; &#125; &#125; return sum_1 + sum_2 + sum_3&#125; 上面示例3中的代码，第一段代码是常量级的执行时间，第二段代码的时间复杂度是 O(n)，第三段代码的时间复杂度是 O(n^2)。综合这三段代码的时间复杂度，我们取其中最大的量级。所以整段代码的时间复杂度是 O(n^2)。 总的时间复杂度等于量级最大的那段代码的时间复杂度。抽象成公式如下： 如果 T1(n) = O(f(n))，T2(n) = O(g(n))，那么 T(n) = T1(n) + T2(n) = max(O(f(n)), O(g(n))) = O(max(f(n), g(n)))。 乘法法则 1234567891011121314151617// 示例4int cal(int n) &#123; int ret = 0; int i = 1; for(; i &lt; n; ++i) &#123; ret = ret + f(i); &#125;&#125;int f(int n) &#123; int sum = 0; int i = 1; for(; i &lt; n; ++i) &#123; sum = sum + i; &#125; return sum;&#125; 可以把乘法法则看成是嵌套循环。抽象成公式如下： 如果 T1(n) = O(f(n))，T2(n) = O(g(n))，那么 T(n) = T1(n) * T2(n) = O(f(n)) * O(g(n)) = O(f(n) * g(n))。 上面示例4整块代码的时间复杂度可表示为 O(n^2)。 几种常见时间复杂度实例分析 上图中罗列的复杂度量级，可以粗略地分为两类，多项式量级和非多项式量级。 其中非多项式量级只有：O(2^n) 和 O(n!)。当数据规模 n 越来越大时，非多项式量级算法的执行时间急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法是非常低效的算法。 常见多项式时间复杂度 O(1) O(1) 只是常量级时间复杂度的一种表示方法。一般情况下，只要算法中不存在循环语句、递归语句，即便成千上万行的代码，其时间复杂度也是 O(1)。 O(logn)、O(nlogn) 12345// 示例5int i = 1;while(i &lt;= n) &#123; i = i * 2;&#125; 从上面示例5代码中可以看出，变量 i 的值从 1 开始取，每循环一次就乘以 2，当大于 n 时，循环结束。实际上变量 i 的取值就是一个等比数列： 2^1 2^2 2^3 ... 2^x = n 所以，我们只要知道 x 值是多少，就知道这行代码执行的次数了。求解 2^x = n 得到 x = log2N，所以这段代码的时间复杂度就是 O(log2N)。 实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。因为对数之间是可以互相转换的。 如果一段代码的时间复杂度为 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了。归并排序、快速排序的时间复杂度都是 O(nlogn)。 O(m+n)、O(m*n) 另一种不一样的时间复杂度，代码的时间复杂度由两个数据的规模来决定。 12345678910111213141516// 示例6int cal(int m, int n) &#123; int sum_1 = 0; int i = 1; for(; i &lt; m; ++i) &#123; sum_1 = sum_1 + i; &#125; int sum_2 = 0; int j = 1; for(; j &lt; n; ++j) &#123; sum_2 = sum_2 + j; &#125; return sum_1 + sum_2;&#125; 上面示例6的代码中，m 和 n 是表示两个数据规模，我们无法事先评估 m 和 n 谁的量极大，所以在表示时间复杂度的时候，就不能简单地利用加法法则省略掉其中一个，所以该代码的时间复杂度就是 O(m+n)。 如果 T1(n) = O(f(n))，T2(n) = O(g(n))，那么 加法法则： T(n) = T1(n) + T2(n) = O(f(n) + g(n))； 乘法法则：T(n) = T1(n) * T2(n) = O(f(n) * g(n))。 空间复杂度分析空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 12345678// 示例7void print(int n) &#123; int i = 0; int[] a = new int[n]; for(; i &lt; n; ++i) &#123; a[i] = i * i; &#125; &#125; 上面示例7代码中，第三行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 我们常见的空间复杂度就是 O(1)、O(n)、O(n^2)，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。 性能测试与复杂度分析之间的关系复杂度分析为我们提供了一个很好的理论分析的方向，使抽象的性能有了一个直观的表达。它虽然只是一个粗略的分析，但是可以和性能测试没有冲突，两者相辅相成。重点在于在编程的过程中，要具有这种复杂度分析的思维。 内容小结复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率和数据规模之间的增长关系。可以粗略地表示，越高阶复杂度的算法，执行效率越低。 常见的复杂度，从低阶到高阶有：O(1)、O(n)、O(n^2)、O(logn) 和 o(nlogn)。","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"},{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"},{"name":"复杂度分析","slug":"复杂度分析","permalink":"http://yoursite.com/tags/复杂度分析/"}]},{"title":"架构设计的宏观视角","slug":"framework-macro","date":"2019-05-05T03:22:58.000Z","updated":"2019-05-08T07:28:14.875Z","comments":false,"path":"post/framework-macro/","link":"http://yoursite.com/post/framework-macro/","permalink":"http://yoursite.com/post/framework-macro/","excerpt":"现在包罗万象的信息世界，正是在最底层的冯·诺依曼的基础体系上搭建而成，我们熟知的架构，则是最上层的业务架构。","text":"现在包罗万象的信息世界，正是在最底层的冯·诺依曼的基础体系上搭建而成，我们熟知的架构，则是最上层的业务架构。 应用程序的基础架构电脑的工作原理所有的电脑都可以统一看作由”中央处理器 + 存储 + 一系列的输入输出设备“构成。 中央处理器（CPU）：负责按指令执行命令； 存储：负责保存数据，包括我们要执行的指令（以数据的形式保存在存储中）。 为什么电脑能够完成这么多复杂而多样化的工作？ 整个过程依赖以下两点： 可编程性。CPU 指令是一个有限的指令集（计算类、I/O类、指令跳转类），但是 CPU 执行的指令序列（程序）并不是固定的，而是依赖保存在存储中的数据（程序员编写的软件）来决定的。指令序列的可能性是无穷的，这就意味着电脑能够做的事情也是无穷的。 开放设计的外部设备支持。CPU 并不理解外部设备（键盘、打印机、屏幕等等）具体有什么样的能力，它只和这些设备交换数据。 电脑的 CPU 是一个非常简洁的模型，它只读入和写出数据，对数据进行计算。 程序的运行原理有了上面基础的计算机体系，接下来我们就可以编写软件了。 编程语言+编译器 的出现。编译器负责把我们人类容易理解的语言，转换为机器可以理解的机器指令。汇编语言的编译器将汇编语言写的程序翻译成为 CPU 指令序列，并将其保存到外置的存储设备上。 多个软件在同一个电脑上怎么共处？ 多个软件大家往同一个存储地址写数据冲突？一起往打印机发送指令怎么办？等等 操作系统 就出现了。它首先要解决的是软件治理的问题，包括建立安全保护机制和建立软件之间的协作秩序；其次解决的是基础编程接口的问题，一方面简化了软件开发，另一方面提供了多软件共存（多任务）的环境。 像上面所说的冯·诺依曼计算机体系、操作系统和编程语言，这些都是我们开发一个应用程序所依赖的基础架构。基础架构解决的是与业务无关的一些通用性的问题。而且，基础架构通常以独立的软件存在，所以也成为基础软件。 Linux、Nginx、MySQL、PHP 等这些软件都属于基础软件，这些基础软件极大的降低了应用开发的难度。在今天软件服务化的大趋势下，很多基础软件最终以互联网服务的方式提供，这就是所谓的“云计算”。 应用程序的业务架构（应用架构）业务架构虽然会因为应用的领域不同而有很大的差异，但不同业务架构之间，仍然会有许多共通的地方。它们不止遵循相同的架构原则，还可以遵循相同的设计范示（交互领域-MVC、JS-Angular 等）。 完整的程序架构服务端 应用程序完整的程序架构 客户端 应用程序完整的程序架构 内容小结所有的电脑都可以看作由“中央处理器 + 存储 + 一系列输入输出设备”构成。中央处理器（CPU）负责指令执行命令；存储负责保存数据，包括要执行的命令。","categories":[],"tags":[{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"},{"name":"架构设计","slug":"架构设计","permalink":"http://yoursite.com/tags/架构设计/"}]}]}
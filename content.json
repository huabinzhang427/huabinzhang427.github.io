{"meta":{"title":"张华宾的博客","subtitle":"先博而后渊","description":null,"author":"ZhangHuaBin","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"分析冯·诺依曼体系结构","slug":"framework-von-neumann","date":"2019-05-08T01:40:48.000Z","updated":"2019-05-08T07:37:46.213Z","comments":false,"path":"post/framework-von-neumann/","link":"http://yoursite.com/post/framework-von-neumann/","permalink":"http://yoursite.com/post/framework-von-neumann/","excerpt":"冯·诺依曼体系结构的不凡之处在于，它想“解决一切可以用‘计算’来解决的问题”。","text":"冯·诺依曼体系结构的不凡之处在于，它想“解决一切可以用‘计算’来解决的问题”。 分析架构的关键点 我们应该如何去分析架构设计中涉及的每一个零部件？ 换句话说，当我们设计或分析一个零部件时，我们会关心哪些问题？ 需求这个零部件的作用是什么？它能用来做哪些事情？它不会被用来做哪些事情？ 为何这个零部件被设计用来干这些事情，而不是多干一点事情，或少干某些事情？ 规格这个零部件的接口是什么样的？它如何与其他零部件连接在一起的？ 规格是零部件的连接需求的抽象。符合规格的零部件可以有非常多种可能的实现方案，但是，一旦规格中的某个条件不能满足了，它就无法正常完成与其他零部件的连接，以达到预期的需求目标。 规格的约束条件是非常多样化的。可能是外观（如形状和颜色）、交互方式（如键盘、鼠标、语音、触摸屏）、质量（硬度、耐热性等）。 那么，冯·诺依曼体系结构的需求和规格又是什么呢？ 冯·诺依曼体系结构分析冯·诺依曼体系结构不但是应用程序这座大厦的地基，同时也是整个信息科技的地基。事实上，它更像是一个无中生有的全新世界：在其中，有个体、有族群、有生态，还有喜怒哀乐。 从需求来说，它想解决一切问题。解决一切可以用“计算”来解决的问题。 为了实现“解决一切可以用‘计算’来解决的问题”这个目标，冯·诺依曼引入了三类基础零部件： 中央处理器 存储 输入输出设备 存储，它负责存放计算涉及的相关数据，作为计算的输入参数和输出结果。常见的存储设备非常多样化，比如：中央处理器内置的寄存器、内存、传统机械硬盘、USB 固态硬盘等等。 从中央处理器的角度，存储见到那分为两类： 内置支持的存储（常规的处理器指令可直接访问），比如寄存器、内存、计算机主板的 ROM。 外置存储（中央处理器本身并不能直接读写其中的数据），输入输出设备。 冯·诺依曼体系涉及的“存储”，指的是中央处理器内置支持的存储。 输入输出设备，每个设备通过一个端口与中央处理器连接。通过这个端口地址，中央处理器可以和设备进行数据交换。 数据交换设计的数据格式由设备定义，中央处理器并不理解。设备数据交换的发起方（设备使用方）通常理解并可以解释所接收的数据含义。为了方便使用，设备厂商或操作系统厂商通常会提供设备相关的驱动程序，把设备交换的细节隐藏起来，设备的使用方只需要调用相关的接口函数就可以操作设备。 中央处理器，它负责程序（指令序列）的执行。指令序列也存放在存储里面，计算机加电启动后，中央处理器会从一个固定的存储地址开始执行。通常这个固定的存储地址指向计算机主板的 ROM 上的一段启动程序（BIOS），这段启动程序通常包含以下内容： 存储设备的驱动程序，用以识别常规的外置存储设备，比如硬盘、光驱、U 盘； 基础外部设备的驱动程序，比如键盘、鼠标、显示器（显卡）； 设备和启动配置的基础管理能力； 在外置存储上执行程序的能力； 将执行权转移到外置存储上的操作系统启动程序，这样操作系统就开始干活了。 “计算”需求的多样性只需要通过调整计算机主板上的 BIOS 程序，乃至外置存储中的操作系统启动程序就可以实现了，而不必去修改中央处理器本身。 中央处理器支持的指令分类： 计算类，各类数学运算； I/O 类，从存储读写数据，从输入输出设备读写数据； 指令跳转类，在满足特定条件下跳转到新的当前程序执行位置、调用自定义函数。 和“解决一切可以用‘计算’来解决的问题”这个伟大的目标相比，冯·诺依曼体系的三类零部件的规格设计显得如此精简。 为什么这么简洁的规格设计，居然可以解决这么复杂的需求？ 需求是怎么被满足的？对于“解决一切可以用‘计算’来解决的问题”这样一个模糊的需求，如果你是“电脑”这个产品的主架构师，你会如何应对？ 分析思路如下： 一方面，需求的变化点在于，要解决的问题是五花八门包罗万象的。如何以某种稳定但可扩展的架构来支持这样的变化呢？ 另一方面，需求的稳定点在于，电脑的核心能力是固定的，怎么表达电脑的核心能力？ 电脑的核心能力是“计算”。 什么是计算？计算就是对一个数据（输入）进行变换，变为另一个数据（输出）。在数学中把它叫作“函数”。 y = f(x) x、y 是数据，可以是简单的数值，也可能是文本、图片、视频，各种我们对现实问题进行参数化建模后的测量值，当然也可能是多个输入数据。但无论它的逻辑含义是什么，物理上都可以以一段连续的字节内容来表达。 x、y 物理上在哪里？ “存储”这个概念就产生了。存储就是存放计算所要操作数据的所在。 一个具体的计算（函数）怎么表达？ 计算（函数）对于电脑的架构师来说是未知的，那么怎么设计一种系统架构让用户可以表达任意复杂的计算（函数）？ 逻辑上来看，无论多么复杂的自定义函数，都可以通过下面这些元素的组合来定义： 内置函数，整数或小数运算 循环和条件分支 子函数（自定义函数） 因此，对于任意的具体的计算（自定义函数）来说，都可以用一组指令序列来表达。 那么函数物理上在哪里？ 函数以指令序列形式存放在存储里面。也就是说，存储不只存放计算索要操作的数据，也存放“计算”本身。 只是，存储里面存放的“计算”只是数据，需要有人理解并执行这些数据背后的计算行为，才变成真正意义的“计算”。这个执行者，就是中央处理器（CPU），它支持很多计算指令，包括执行内置函数、循环和条件分支、执行子函数等。 电脑有了“中央处理器+存储”，即有了计算的能力和计算的数据，但无法和现实世界发生交互。 交互，抽象来看就是输入和输出。对于电脑来说，输入和输出的需求多样化。 从输入需求来说，可能采集静态图像、声音、视频；也可能采集结构化的数据，如 GPS 位置、脉搏、心电图、温度、湿度等；还可能是用户控制指令，如键盘按键、鼠标、触摸屏动作等。 从输出需求来说，可能像屏幕输出信息；播放声音；执行某个动作如交通灯开关、打印机打印等。 不管是什么样交互用途的设备，我们要做的只是定义好统一的数据交换协议？ 因此中央处理器不仅要有“计算”能力，还要有“数据交换”能力（IO 能力）。 输入输出设备从根本上解决了什么问题？ 是电脑无限扩展的能力。最重要的是，输入输出设备和电脑是完全异构的。输入输出设备对电脑来说就只是实现了某项功能的黑匣子。如果把交互能力也看作是一种计算能力的话，电脑理论上能够解决的“计算”问题变得无所不包。 内容总结架构的第一步是需求分析，从需求分析角度来说，关键要抓住需求的稳定点和变化点。需求的稳定点，往往是系统的核心价值点；而需求的变化点，则往往需要相应去做开放性设计。 对于电脑这个产品来说，需求的稳定点是电脑的 “计算”能力 ，需求的变化点，一是用户 “计算”需求 的多样性，二是用户交互方式的多样性。 电脑的“计算”能力，最终体现为中央处理器的指令集，这是需求相对稳定的部分。 用户“计算”需求的多样性，只需要通过调整计算机主板上的 BIOS 程序，乃至外置存储中的操作系统启动程序来实现，而不必去修改中央处理器本身。 用户交互方式的多样性，则通过定义外部设备与中央处理器的数据交换协议实现。 分析需求的稳定点和变化点 描述需求需要几个典型的要素： 用户，面向的人群； 他们有什么要解决的问题； 解决这个问题的核心系统 只有满足了上面几个要素的需求才能进一步讨论变化点和稳定点。 存储让数据跨越时间，传输让数据跨越空间，计算让数据改变形式。","categories":[],"tags":[{"name":"架构设计","slug":"架构设计","permalink":"http://yoursite.com/tags/架构设计/"},{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"}]},{"title":"最好/最坏/平均/均摊时间复杂度分析","slug":"complexity-analysis2","date":"2019-05-06T07:13:59.000Z","updated":"2019-05-06T10:54:05.373Z","comments":false,"path":"post/complexity-analysis2/","link":"http://yoursite.com/post/complexity-analysis2/","permalink":"http://yoursite.com/post/complexity-analysis2/","excerpt":"同一段代码，在不同输入的情况下，复杂度量级可能是不一样的。","text":"同一段代码，在不同输入的情况下，复杂度量级可能是不一样的。 最好情况时间复杂度（best case time complexity） 最坏情况时间复杂度（worst case time complexity） 平均情况时间复杂度（average case time complexity） 均摊时间复杂度（amoritized time complexity） 基础复杂度分析回顾12345678910// 示例1// n 表示 array 长度int find(int[] array, int n, int x) &#123; int i = 0; int pos = -1; for(; i &lt; n; ++i) &#123; if(array[i] == x) pos = i; &#125; return pos;&#125; 上面示例1代码的功能是，在一个无序的数组（array）中，查找变量 x 出现的位置，如果没有找到就返回 -1。通过之前的分析方法，该代码算法的时间复杂度是 O(n)。 我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环。所以优化示例1中的代码得到： 12345678910111213// 示例2// n 表示 array 长度int find(int[] array, int n, int x) &#123; int i = 0; int pos = -1; for(; i &lt; n; ++i) &#123; if(array[i] == x) &#123; pos = i; break; &#125; &#125; return pos;&#125; 我们优化完代码后，示例2中的时间复杂度还是 O(n) 吗？ 分析：因为我们要查找的变量 x 可能出现在数组的任意位置。如果数组中的第一个元素正好是要查找的变量 x，那就不需要再继续遍历剩下的 n-1 个数据了，那时间复杂度就是 O(1)。但如果数组中不存在变量 x，那我们就需要把整个数组都遍历一遍，时间复杂度就变成了 O(n)。所以不同的情况下，示例2中的时间复杂度是不一样的。 最好情况时间复杂度最好情况时间复杂度，就是在最理想的情况下，执行代码的时间复杂度。 在示例2中，在最理想的情况下，要查找的变量 x 正好是数组的第一个元素，这个时候对应的时间复杂度 O(1) 就是最好情况时间复杂度。 最坏情况时间复杂度最坏情况时间复杂度，就是在最糟糕的情况下，执行代码的时间复杂度。 在示例2中，在最坏的情况下，要查找的变量 x 不在数组中，我们需要把整个数组都遍历一遍才行，这个时候对应的时间复杂度 O(n) 就是最好情况时间复杂度。 平均情况时间复杂度我们都知道最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码时间复杂度，发生的概率其实并不大。所以引入平均情况时间复杂度，简称平均时间复杂度。 平均时间复杂度怎么分析? 在示例2中，我们要查找的变量 x 在数组中的位置，有 n+1 种情况： 在数组的 0~n-1 位置中 不在数组中 我们把每种情况下，查找需要遍历的元素个数累加起来,然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即： (1+2+3...+n+n)/(n+1) = n(n+3)/2(n+1) 根据时间复杂度大 O 表示法，上面的公式简化后，得到的平均复杂度就是 O(n)。 上面的 n+1 种情况出现的概率并不是一样的，又该怎么处理？ 我们假设在数组中和不在数组中的概率都是 1/2，另外要查找的数据出现在 0~n-1 这 n 个位置的概率是一样的，为 1/n。所以，根据概率乘法原则，要查找的数据出现在 0~n-1 中任意位置的概率就是 1/(2n)。 把各种情况发生的概率考虑进去，那么平均复杂度的计算过程变成了： (1+2+3...+n)*(1/2n)+n*(1/2) = (3n+1)/4 上面的计算结果就是加权平均值（期望值），所以平均时间复杂度的全称叫加权平均时间复杂度或期望时间复杂度。 引入概率后，最终示例2代码的加权平均时间复杂度仍然是 O(n)。 实际上在大多数情况下，我们不需要区分最好、最坏、平均情况时间复杂度三种情况，使用一个复杂度就可以满足要求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距时，才会使用这三种复杂度表示来区分。 均摊时间复杂度12345678910111213141516// 示例3int[] array = new int[n];int count = 0;void insert(int val) &#123; if(count == array.length) &#123; int sum = 0; for(int i = 0; i &lt; array.length; ++i) &#123; sum = sum + array[i]; &#125; array[0] = sum; count = 1; &#125; array[count] = val; ++count;&#125; 上面示例3代码的功能是往数组中插入数据。当数组满了之后，用 for 循环遍历数组求和，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据依次插入。 示例3代码的时间复杂度分析如下： 最理想的情况下，数组中有空闲空间，所以最好情况时间复杂度为 O(1)。最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为 O(n)。 平均时间复杂度呢？ 假设数组的长度为 n，根据数据插入的位置的不同，我们可以分为 n 种情况，每种情况的时间复杂度是 O(1)，除此之外，还有一种情况是，在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是 O(n)，而且这 n+1 种情况发生的概率一样，都是 1/(n+1)。所以根据加权平均的计算方法，得到的期望时间复杂度是： (1+1+1...+n)/(n+1) = O(1) 示例3代码的平均时间复杂度不需要引入概率？ 对比示例2代码中的 find() 函数和示例3代码中的 insert() 函数 find() 函数在极端情况下，复杂度才为 O(1)，但 insert() 函数在大部分情况下，时间复杂度都为 O(1)，只要个别情况下复杂度才比较高，为 O(n)。 insert() 函数，O(1) 复杂度的插入和 O(n) 时间复杂度的插入，出现的频率非常有规律，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。 针对这样的特殊情况，我们引入一种更加简单的分析方法：摊还分析法，通过该分析方法得到的时间复杂度叫均摊时间复杂度。 如何使用摊还分析法来分析算法的均摊时间复杂度？ 继续上面示例3中的代码，每一次 O(n) 的插入操作，都会跟着 n-1 次的 O(1) 的插入操作。所以把耗时多的那次操作均摊到接下来的 n-1 次耗时操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一起分析，看是否能将较高时间复杂度那次操作的耗时，平均到其他那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度的分析场合，一般均摊时间复杂度就等于最好情况时间复杂度。 均摊时间复杂度是一种特殊的平均时间复杂度。 内容小结因为同一段代码，在不同的输入情况下，复杂度量级有可能是不一样的。所以引入了最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度和均摊时间复杂度。 平均情况时间复杂度和均摊时间复杂度基本上是同一个概念，均摊是特殊的平均。在分析时间复杂度是 O(1) 还是 O(n) 的时候，出现 O(1) 的次数远大于出现 O(n) 出现的次数时，平均情况/摊还时间复杂度就是 O(1)。","categories":[],"tags":[{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"},{"name":"复杂度分析","slug":"复杂度分析","permalink":"http://yoursite.com/tags/复杂度分析/"}]},{"title":"基础复杂度分析","slug":"complexity-analysis","date":"2019-05-06T02:51:41.000Z","updated":"2019-05-06T07:25:22.452Z","comments":false,"path":"post/complexity-analysis/","link":"http://yoursite.com/post/complexity-analysis/","permalink":"http://yoursite.com/post/complexity-analysis/","excerpt":"数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行的更快，如何让代码更省存储空间。","text":"数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行的更快，如何让代码更省存储空间。 概述说到数据结构和算法，就一定离不开时间、空间复杂度分析。复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。 为什么需要复杂度分析？ 我们把代码跑一遍，通过统计、监控、就能得到算法执行的时间和占用的内存大小。为什么还需要做时间、空间复杂度分析呢？ 上面说的统计、监控的分析方法也叫作事后统计法，这种统计方法由很大的局限性。 测试结果非常依赖测试环境 测试结果受数据规模的影响很大 我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是下面要介绍的时间、空间复杂度分析方法。 大 O 复杂度表示法算法的执行效率，粗略的的讲，就是算法代码执行的时间。 123456789// 示例1int cal(int n) &#123; int sum = 0; int i = 1; for(; i &lt;= n; ++i) &#123; sum = sum + i; &#125; return sum;&#125; 从 CPU 的角度来看，上面代码的每一行执行着类似的操作：读数据-运算-写数据。假设每行代码执行的时间一样长，所有代码的执行时间 T(n) 与每行代码的执行次数成正比。 大 O 公式T(n) = O(f(n)) T(n)：代码执行的总时间； n：数据规模的大小； f(n)：每行代码执行的次数总和。 所以示例1中代码的大 O 时间复杂度可表示为 T(n) = O(2n + 2)。 大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势。所以，也叫作渐进时间复杂度(asymptotic time complexity)，简称时间复杂度。 当 n 很大时，公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略，我们只需要记录一个最大量级的就可以了。 时间复杂度分析 如何分析一段代码的时间复杂度？ 只需关注循环执行次数最多的一段代码 123456789// 示例2int cal(int n) &#123; int sum = 0; iny i = 1; for(; i &lt;= n; ++i) &#123; sum = sum + 1; &#125; return sum;&#125; 示例2中，第2、3行代码都是常量级的执行时间，与 n 的大小无关，所以对于复杂度没有影响。循环次数最多的是第4、5行代码，这两行代码被执行了 n 次，所以总的时间复杂度就是 O(n)。 加法法则 加法法则，总复杂度等于量级最大的那段代码的复杂度。 12345678910111213141516171819202122232425// 示例3int cal(int n) &#123; int sum_1 = 0; int p = 1; for(; p &lt; 100; ++p) &#123; sum_1 = sum_1 + p; &#125; int sum_2 = 0; int q = 1; for(; q &lt; 100; ++q) &#123; sum_2 = sum_2 + q; &#125; int sum_3 = 0; int i = 1; int j = 1; for(; i &lt; 100; ++i) &#123; int j = 1; for(; j &lt; 100; ++j) &#123; sum_3 = sum_3 + i * j; &#125; &#125; return sum_1 + sum_2 + sum_3&#125; 上面示例3中的代码，第一段代码是常量级的执行时间，第二段代码的时间复杂度是 O(n)，第三段代码的时间复杂度是 O(n^2)。综合这三段代码的时间复杂度，我们取其中最大的量级。所以整段代码的时间复杂度是 O(n^2)。 总的时间复杂度等于量级最大的那段代码的时间复杂度。抽象成公式如下： 如果 T1(n) = O(f(n))，T2(n) = O(g(n))，那么 T(n) = T1(n) + T2(n) = max(O(f(n)), O(g(n))) = O(max(f(n), g(n)))。 乘法法则 1234567891011121314151617// 示例4int cal(int n) &#123; int ret = 0; int i = 1; for(; i &lt; n; ++i) &#123; ret = ret + f(i); &#125;&#125;int f(int n) &#123; int sum = 0; int i = 1; for(; i &lt; n; ++i) &#123; sum = sum + i; &#125; return sum;&#125; 可以把乘法法则看成是嵌套循环。抽象成公式如下： 如果 T1(n) = O(f(n))，T2(n) = O(g(n))，那么 T(n) = T1(n) * T2(n) = O(f(n)) * O(g(n)) = O(f(n) * g(n))。 上面示例4整块代码的时间复杂度可表示为 O(n^2)。 几种常见时间复杂度实例分析 上图中罗列的复杂度量级，可以粗略地分为两类，多项式量级和非多项式量级。 其中非多项式量级只有：O(2^n) 和 O(n!)。当数据规模 n 越来越大时，非多项式量级算法的执行时间急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法是非常低效的算法。 常见多项式时间复杂度 O(1) O(1) 只是常量级时间复杂度的一种表示方法。一般情况下，只要算法中不存在循环语句、递归语句，即便成千上万行的代码，其时间复杂度也是 O(1)。 O(logn)、O(nlogn) 12345// 示例5int i = 1;while(i &lt;= n) &#123; i = i * 2;&#125; 从上面示例5代码中可以看出，变量 i 的值从 1 开始取，每循环一次就乘以 2，当大于 n 时，循环结束。实际上变量 i 的取值就是一个等比数列： 2^1 2^2 2^3 ... 2^x = n 所以，我们只要知道 x 值是多少，就知道这行代码执行的次数了。求解 2^x = n 得到 x = log2N，所以这段代码的时间复杂度就是 O(log2N)。 实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。因为对数之间是可以互相转换的。 如果一段代码的时间复杂度为 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了。归并排序、快速排序的时间复杂度都是 O(nlogn)。 O(m+n)、O(m*n) 另一种不一样的时间复杂度，代码的时间复杂度由两个数据的规模来决定。 12345678910111213141516// 示例6int cal(int m, int n) &#123; int sum_1 = 0; int i = 1; for(; i &lt; m; ++i) &#123; sum_1 = sum_1 + i; &#125; int sum_2 = 0; int j = 1; for(; j &lt; n; ++j) &#123; sum_2 = sum_2 + j; &#125; return sum_1 + sum_2;&#125; 上面示例6的代码中，m 和 n 是表示两个数据规模，我们无法事先评估 m 和 n 谁的量极大，所以在表示时间复杂度的时候，就不能简单地利用加法法则省略掉其中一个，所以该代码的时间复杂度就是 O(m+n)。 如果 T1(n) = O(f(n))，T2(n) = O(g(n))，那么 加法法则： T(n) = T1(n) + T2(n) = O(f(n) + g(n))； 乘法法则：T(n) = T1(n) * T2(n) = O(f(n) * g(n))。 空间复杂度分析空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 12345678// 示例7void print(int n) &#123; int i = 0; int[] a = new int[n]; for(; i &lt; n; ++i) &#123; a[i] = i * i; &#125; &#125; 上面示例7代码中，第三行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 我们常见的空间复杂度就是 O(1)、O(n)、O(n^2)，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。 性能测试与复杂度分析之间的关系复杂度分析为我们提供了一个很好的理论分析的方向，使抽象的性能有了一个直观的表达。它虽然只是一个粗略的分析，但是可以和性能测试没有冲突，两者相辅相成。重点在于在编程的过程中，要具有这种复杂度分析的思维。 内容小结复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率和数据规模之间的增长关系。可以粗略地表示，越高阶复杂度的算法，执行效率越低。 常见的复杂度，从低阶到高阶有：O(1)、O(n)、O(n^2)、O(logn) 和 o(nlogn)。","categories":[],"tags":[{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"},{"name":"复杂度分析","slug":"复杂度分析","permalink":"http://yoursite.com/tags/复杂度分析/"}]},{"title":"架构设计的宏观视角","slug":"framework-macro","date":"2019-05-05T03:22:58.000Z","updated":"2019-05-08T07:28:14.875Z","comments":false,"path":"post/framework-macro/","link":"http://yoursite.com/post/framework-macro/","permalink":"http://yoursite.com/post/framework-macro/","excerpt":"现在包罗万象的信息世界，正是在最底层的冯·诺依曼的基础体系上搭建而成，我们熟知的架构，则是最上层的业务架构。","text":"现在包罗万象的信息世界，正是在最底层的冯·诺依曼的基础体系上搭建而成，我们熟知的架构，则是最上层的业务架构。 应用程序的基础架构电脑的工作原理所有的电脑都可以统一看作由”中央处理器 + 存储 + 一系列的输入输出设备“构成。 中央处理器（CPU）：负责按指令执行命令； 存储：负责保存数据，包括我们要执行的指令（以数据的形式保存在存储中）。 为什么电脑能够完成这么多复杂而多样化的工作？ 整个过程依赖以下两点： 可编程性。CPU 指令是一个有限的指令集（计算类、I/O类、指令跳转类），但是 CPU 执行的指令序列（程序）并不是固定的，而是依赖保存在存储中的数据（程序员编写的软件）来决定的。指令序列的可能性是无穷的，这就意味着电脑能够做的事情也是无穷的。 开放设计的外部设备支持。CPU 并不理解外部设备（键盘、打印机、屏幕等等）具体有什么样的能力，它只和这些设备交换数据。 电脑的 CPU 是一个非常简洁的模型，它只读入和写出数据，对数据进行计算。 程序的运行原理有了上面基础的计算机体系，接下来我们就可以编写软件了。 编程语言+编译器 的出现。编译器负责把我们人类容易理解的语言，转换为机器可以理解的机器指令。汇编语言的编译器将汇编语言写的程序翻译成为 CPU 指令序列，并将其保存到外置的存储设备上。 多个软件在同一个电脑上怎么共处？ 多个软件大家往同一个存储地址写数据冲突？一起往打印机发送指令怎么办？等等 操作系统 就出现了。它首先要解决的是软件治理的问题，包括建立安全保护机制和建立软件之间的协作秩序；其次解决的是基础编程接口的问题，一方面简化了软件开发，另一方面提供了多软件共存（多任务）的环境。 像上面所说的冯·诺依曼计算机体系、操作系统和编程语言，这些都是我们开发一个应用程序所依赖的基础架构。基础架构解决的是与业务无关的一些通用性的问题。而且，基础架构通常以独立的软件存在，所以也成为基础软件。 Linux、Nginx、MySQL、PHP 等这些软件都属于基础软件，这些基础软件极大的降低了应用开发的难度。在今天软件服务化的大趋势下，很多基础软件最终以互联网服务的方式提供，这就是所谓的“云计算”。 应用程序的业务架构（应用架构）业务架构虽然会因为应用的领域不同而有很大的差异，但不同业务架构之间，仍然会有许多共通的地方。它们不止遵循相同的架构原则，还可以遵循相同的设计范示（交互领域-MVC、JS-Angular 等）。 完整的程序架构服务端 应用程序完整的程序架构 客户端 应用程序完整的程序架构 内容小结所有的电脑都可以看作由“中央处理器 + 存储 + 一系列输入输出设备”构成。中央处理器（CPU）负责指令执行命令；存储负责保存数据，包括要执行的命令。","categories":[],"tags":[{"name":"架构设计","slug":"架构设计","permalink":"http://yoursite.com/tags/架构设计/"},{"name":"摘录","slug":"摘录","permalink":"http://yoursite.com/tags/摘录/"}]}]}